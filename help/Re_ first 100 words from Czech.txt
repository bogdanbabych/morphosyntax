From: Serge Sharoff <s.sharoff@leeds.ac.uk>
Subject: Re: first 100 words from Czech
Date: 15 November 2016 09:24:03 GMT
To: Bogdan Babych <B.Babych@leeds.ac.uk>

The form frequency list I sent also has capitalisation issues.  I haven't 
corrected them, since the main reason for using it is to get a list of 
candidates.  You can simply do case-insensitive comparison.

As for using cwb-scan-corpus, the other problem is with elaborate tags, while 
we only need the top-level POS.  I think the easier option to get them into a 
frequency lis is via:
cwb-decode -C INTERNET-UA -P word -P pos | sed -r 's/\t([A-Z]).+/\t\1/' | gawk 
'{print(tolower($0))}' | sort | uniq -c | sort -nr -k 1 >uk-forms.num

Serge

On Tuesday, 15 November 2016 08:58:05 GMT Bogdan Babych wrote:
Thanks, Sergei!

I will work with your file — 
I tried yesterday to generate the same set with the command:
cwb-scan-corpus -C INTERNET-UA word pos | sort -nr -k 1
— but this gave me lots of capitalisation issues, I did not know how to
lowercase everything systematically before frequency is computed;
I see
that you solved this problem somehow?

PoS issues may be interfering: now I am using PoS filter to allow only
cognates with the same PoS class; a possibility would be to use PoS codes
from the INTERNET-UA and map open-class categories to the Czech and Russian
tag sets if this becomes a problem.

(I think also we can use uk-ru glossary which I have made to calibrate the
method — for which settings gets the biggest number of hits if the word
form = lemma which is found in the dictionary.)


Bogdan







Dr Bogdan Babych
Associate Professor in Translation Studies
University of Leeds, Leeds LS2 9JT, UK
tel.: +44(0)113 343 1085
b.babych@leeds.ac.uk

On 15 Nov 2016, at 08:46, Serge Sharoff <s.sharoff@leeds.ac.uk> wrote:


If you want, I've prepared:
http://corpus.leeds.ac.uk/frqc/i-uk-100k.num

(it's a bit different as it doesn't have the POS tags, but I guess they
can be 
ignored at this stage).

On Monday, 14 November 2016 22:23:31 GMT Bogdan Babych wrote:

Thanks, Sergei,

I will try to generate Ukrainian num file for word forms and see what is
ru-uk and cz-uk performance. 

I see that normalised distances above 0.25

are dangerous, and insertion cost could be made a bit higher. There are
some examples where edit distance exists for a straightforward
transliteration, maybe something is wrong with features or some
characters
do not match. 
There is also an issue with computation time, but that also can be
resolved
somehow, with optimising the code.




Bogdan


Dr Bogdan Babych
Associate Professor in Translation Studies
University of Leeds, Leeds LS2 9JT, UK
tel.: +44(0)113 343 1085
b.babych@leeds.ac.uk

On 14 Nov 2016, at 21:08, Serge Sharoff <s.sharoff@leeds.ac.uk> wrote:



thanks.  Probably we're more interested in better recall, but the
danger
is 

that the output can become less and less sensible.  So far, I see

that the results are quite far from success:
nejsou  VERB    frq=424         написано        VERB    33     
PhLev=2.40


PhLevNormLen=0.30

republiky       NOUN    frq=424         республик       NOUN    22     

PhLev=2.20      PhLevNormLen=0.24
možnost NOUN    frq=423         минут   NOUN    74      PhLev=1.60     

PhLevNormLen=0.23
méně    ADV     frq=422         менее   ADV     180     PhLev=0.80     

PhLevNormLen=0.16
září    NOUN    frq=422         задачи  NOUN    94      PhLev=1.20     

PhLevNormLen=0.20
století NOUN    frq=421         столетия        NOUN    35     
PhLev=0.60


PhLevNormLen=0.07

během   ADP     frq=416         помимо  ADP     71      PhLev=2.00     

PhLevNormLen=0.33
snad    ADV     frq=416         значит  ADV     140     PhLev=1.20     

PhLevNormLen=0.20
státu   NOUN    frq=415         статус  NOUN    44      PhLev=0.40     

PhLevNormLen=0.07
návrh   NOUN    frq=414         ни      NOUN    21      PhLev=1.60     

PhLevNormLen=0.32


However, why Czech-Russian?  The idea is to map the Czech and Russian 
resources into Ukrainian to train a noisy tagger/parser, so we need a
Cz-Uk 

mapping (as well as Ru-Uk).


Serge

On Monday, 14 November 2016 19:25:36 GMT Bogdan Babych wrote:


And we have 300 words now:
cs-ru-crosslevV03TopCzechFirst300words.txt

https://www.dropbox.com/s/sr1omvge94v2pe1/cs-ru-crosslevV03TopCzechFirs
t3
00w

ords.txt?dl=0


— I am printing edit distances, also normalised by length, much noise,
but
we can use a threshold and only output reliable pairs Do we need
higher
recall, or precision?


Bogdan


Dr Bogdan Babych
Associate Professor in Translation Studies
University of Leeds, Leeds LS2 9JT, UK
tel.: +44(0)113 343 1085
b.babych@leeds.ac.uk

On 14 Nov 2016, at 16:01, Bogdan Babych <B.Babych@leeds.ac.uk> wrote:

— in morpho in the file:
cs-ru-crosslevV03TopCzech100words.txt

— more to come, some issues with accuracy, although….

bogdan


Dr Bogdan Babych
Associate Professor in Translation Studies
University of Leeds, Leeds LS2 9JT, UK
tel.: +44(0)113 343 1085
b.babych@leeds.ac.uk











